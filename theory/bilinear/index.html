<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Bilinear Forms and Metric Tensors | Attention as Bilinear Form</title><meta content="A Physicist's Guide to Transformer Attention" name=description><link href=https://planckeon.github.io/attn-as-bilinear-form/style.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css rel=stylesheet><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });" defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><body><header><nav><a class=logo href=https://planckeon.github.io/attn-as-bilinear-form> <span class=logo-icon>∑</span> <span class=logo-text>Attention as Bilinear Form</span> </a><div class=nav-links><a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>GitHub</a></div></nav></header><main><article class="content page"><h1>Bilinear Forms and Metric Tensors</h1><h2 id=mathematical-foundation>Mathematical Foundation</h2><p>A <strong>bilinear form</strong> is a map $B: V \times W \to \mathbb{R}$ that is linear in both arguments.<p>In index notation:<p>$$B(u, v) = u^a g_{ab} v^b$$<p>where $g_{ab}$ is a <strong>metric tensor</strong>.<h2 id=index-conventions>Index Conventions</h2><p>We use physics conventions throughout:<table><thead><tr><th>Notation<th>Meaning<tbody><tr><td>$u^a$<td>Contravariant vector (upper index)<tr><td>$u_a$<td>Covariant vector (lower index)<tr><td>$g_{ab}$<td>Metric tensor (lowers indices)<tr><td>$g^{ab}$<td>Inverse metric (raises indices)</table><p><strong>Einstein summation</strong>: Repeated indices (one up, one down) are summed:<p>$$u^a v_a = \sum_{a=1}^{d} u^a v_a$$<h2 id=metric-tensors>Metric Tensors</h2><h3 id=euclidean-metric>Euclidean Metric</h3><p>$$g_{ab} = \delta_{ab}$$<p>This gives the standard dot product:<p>$$\langle u, v \rangle = u^a \delta_{ab} v^b = u^a v^a$$<h3 id=scaled-euclidean-metric>Scaled Euclidean Metric</h3><p>$$g_{ab} = \frac{1}{\sqrt{d}} \delta_{ab}$$<p>This is the metric used in standard attention (Vaswani et al., 2017). The $1/\sqrt{d_k}$ scaling prevents dot products from growing too large in high dimensions.<h3 id=learned-metric>Learned Metric</h3><p>$$g_{ab} = (W^T W)_{ab}$$<p>For a weight matrix $W$, this ensures the metric is <strong>positive semi-definite</strong>.<h2 id=index-raising-and-lowering>Index Raising and Lowering</h2><p><strong>Lowering an index</strong> (contravariant → covariant):<p>$$v_a = g_{ab} v^b$$<p><strong>Raising an index</strong> (covariant → contravariant):<p>$$v^a = g^{ab} v_b$$<p>where $g^{ab}$ is the inverse metric satisfying:<p>$$g^{ac} g_{cb} = \delta^a_b$$<h2 id=properties-of-valid-metrics>Properties of Valid Metrics</h2><p>A valid metric tensor must be:<ol><li><strong>Symmetric</strong>: $g_{ab} = g_{ba}$<li><strong>Positive definite</strong>: $v^a g_{ab} v^b > 0$ for all $v \neq 0$</ol><p>The eigenvalues of $g$ must all be positive.<h2 id=connection-to-attention>Connection to Attention</h2><p>In attention, the score between query $i$ and key $j$ is:<p>$$S^{ij} = Q^{ia} g_{ab} K^{jb}$$<p>For standard attention:<p>$$g_{ab} = \frac{1}{\sqrt{d_k}} \delta_{ab}$$<p>This is why the formula has $\frac{QK^T}{\sqrt{d_k}}$ — the metric tensor is embedded in the scaling!<h2 id=code-example>Code Example</h2><pre><code data-lang=python>from attn_tensors.bilinear import (
    euclidean_metric,
    scaled_euclidean_metric,
    bilinear_form,
    bilinear_form_batch,
)

# Create a metric
d = 64
g = scaled_euclidean_metric(d)  # shape: (64, 64)

# Compute bilinear form for a single pair
u = jnp.ones(d)
v = jnp.ones(d)
result = bilinear_form(u, v, g)  # scalar

# Batch computation for attention scores
Q = jnp.randn(10, d)  # 10 queries
K = jnp.randn(20, d)  # 20 keys
scores = bilinear_form_batch(Q, K, g)  # shape: (10, 20)
</code></pre><h2 id=worked-example-computing-bilinear-forms>Worked Example: Computing Bilinear Forms</h2><p>Let's compute attention scores step-by-step with a tiny example.<p><strong>Setup:</strong><ul><li>Dimension $d = 3$<li>Query: $q = [1, 2, 1]$<li>Key: $k = [2, 1, 0]$<li>Metric: $g = \frac{1}{\sqrt{3}} I_3$ (scaled identity)</ul><p><strong>Step 1: Write out the metric tensor</strong><p>$$g_{ab} = \frac{1}{\sqrt{3}} \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$$<p><strong>Step 2: Compute the bilinear form</strong><p>$$B(q, k) = q^a g_{ab} k^b = \frac{1}{\sqrt{3}} \sum_{a=1}^{3} q^a k^a$$<p>$$= \frac{1}{\sqrt{3}} (1 \cdot 2 + 2 \cdot 1 + 1 \cdot 0)$$<p>$$= \frac{1}{\sqrt{3}} (2 + 2 + 0) = \frac{4}{\sqrt{3}} \approx 2.31$$<p><strong>Interpretation:</strong> This is the attention score between this query-key pair.<h2 id=generalized-metrics-learning-similarity>Generalized Metrics: Learning Similarity</h2><h3 id=mahalanobis-distance>Mahalanobis Distance</h3><p>A learned metric $g_{ab} = (W^T W)_{ab}$ gives:<p>$$B(q, k) = q^T W^T W k = (Wq)^T (Wk)$$<p>This computes dot product in a transformed space!<h3 id=asymmetric-bilinear-forms>Asymmetric Bilinear Forms</h3><p>We can also use non-symmetric matrices:<p>$$B(q, k) = q^T M k$$<p>where $M$ is not necessarily symmetric. This allows different "meanings" for queries vs keys.<h3 id=connection-to-kernel-methods>Connection to Kernel Methods</h3><p>The bilinear form defines a kernel:<p>$$K(q, k) = \exp(q^T g k)$$<p>This is a valid Mercer kernel when $g$ is positive definite.<h2 id=differential-geometry-perspective>Differential Geometry Perspective</h2><h3 id=tangent-vectors-and-cotangent-vectors>Tangent Vectors and Cotangent Vectors</h3><p>In differential geometry:<ul><li><strong>Contravariant vectors</strong> $v^a$: Tangent vectors (directions)<li><strong>Covariant vectors</strong> $u_a$: Cotangent vectors (linear functionals)</ul><p>The metric converts between them:<ul><li>Lower: $v_a = g_{ab} v^b$<li>Raise: $v^a = g^{ab} v_b$</ul><h3 id=musical-isomorphisms>Musical Isomorphisms</h3><p>In differential geometry notation:<ul><li>$\flat$ (flat): Lowers index, $v^\flat = g(v, \cdot)$<li>$\sharp$ (sharp): Raises index, $\omega^\sharp = g^{-1}(\omega, \cdot)$</ul><h3 id=inner-product-structure>Inner Product Structure</h3><p>The metric defines an inner product on the tangent space:<p>$$\langle u, v \rangle_g = u^a g_{ab} v^b$$<p>This measures "lengths" and "angles" in feature space.</article></main><footer><p>Built with <a href=https://www.getzola.org/ rel=noopener target=_blank>Zola</a> · <a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>Source on GitHub</a></footer>