<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Multi-Head Attention | Attention as Bilinear Form</title><meta content="A Physicist's Guide to Transformer Attention" name=description><link href=https://planckeon.github.io/attn-as-bilinear-form/style.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css rel=stylesheet><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });" defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><body><header><nav><a class=logo href=https://planckeon.github.io/attn-as-bilinear-form> <span class=logo-icon>∑</span> <span class=logo-text>Attention as Bilinear Form</span> </a><div class=nav-links><a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>GitHub</a></div></nav></header><main><article class="content page"><h1>Multi-Head Attention</h1><h2 id=why-multiple-heads>Why Multiple Heads?</h2><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.<p>With a single head, averaging over positions inhibits this. Multiple heads provide independent "views" of the sequence.<h2 id=index-notation-for-multi-head>Index Notation for Multi-Head</h2><p>Introduce head index $h \in {1, \ldots, H}$.<h3 id=input-projections>Input Projections</h3><p>Starting from input $X^{ib}$ (position $i$, feature $b$):<p>$$Q^{hia} = X^{ib} W_Q^{hba}$$ $$K^{hja} = X^{jb} W_K^{hba}$$ $$V^{hjc} = X^{jb} W_V^{hbc}$$<p>where:<ul><li>$W_Q^{hba}$: Query projection for head $h$ (shape: $d_{model} \times d_k$)<li>$W_K^{hba}$: Key projection for head $h$ (shape: $d_{model} \times d_k$)<li>$W_V^{hbc}$: Value projection for head $h$ (shape: $d_{model} \times d_v$)</ul><h3 id=per-head-attention>Per-Head Attention</h3><p>Each head computes attention independently:<p><strong>Scores:</strong> $$S^{hij} = \frac{1}{\sqrt{d_k}} Q^{hia} K^{hja}$$<p><strong>Attention weights:</strong> $$A^{hij} = \frac{\exp(S^{hij})}{\sum_{j'} \exp(S^{hij'})}$$<p><strong>Per-head output:</strong> $$O^{hic} = A^{hij} V^{hjc}$$<h3 id=concatenation-and-output-projection>Concatenation and Output Projection</h3><p>Concatenate all heads and project:<p>$$Y^{id} = O^{hic} W_O^{hcd}$$<p>where $W_O^{hcd}$ has shape $(H \times d_v) \times d_{model}$.<p>The sum over $h$ and $c$ combines all heads.<h2 id=parameter-count>Parameter Count</h2><p>For a transformer with:<ul><li>$d_{model}$: Model dimension<li>$H$: Number of heads<li>$d_k = d_v = d_{model}/H$: Per-head dimension</ul><p>Parameters per layer:<ul><li>$W_Q, W_K, W_V$: Each $d_{model} \times d_{model}$<li>$W_O$: $d_{model} \times d_{model}$<li><strong>Total</strong>: $4 \cdot d_{model}^2$</ul><h2 id=gradient-w-r-t-projection-weights>Gradient w.r.t. Projection Weights</h2><h3 id=gradient-w-r-t-w-q>Gradient w.r.t. $W_Q$</h3><p>Using chain rule:<p>$$\frac{\partial L}{\partial W_Q^{hba}} = \frac{\partial L}{\partial Q^{hia}} \frac{\partial Q^{hia}}{\partial W_Q^{hba}}$$<p>Since $Q^{hia} = X^{ib} W_Q^{hba}$:<p>$$\frac{\partial Q^{h'i'a'}}{\partial W_Q^{hba}} = \delta^{h'}_h X^{i'b} \delta^{a'}_a$$<p>Therefore:<p>$$\frac{\partial L}{\partial W_Q^{hba}} = X^{ib} \frac{\partial L}{\partial Q^{hia}}$$<p><strong>Matrix form</strong> (for head $h$): $\frac{\partial L}{\partial W_Q^h} = X^T \frac{\partial L}{\partial Q^h}$<h3 id=gradient-w-r-t-w-o>Gradient w.r.t. $W_O$</h3><p>From $Y^{id} = O^{hic} W_O^{hcd}$:<p>$$\frac{\partial L}{\partial W_O^{hcd}} = O^{hic} \frac{\partial L}{\partial Y^{id}}$$<h2 id=geometric-view-subspace-projections>Geometric View: Subspace Projections</h2><p>Each head projects queries and keys into a $d_k$-dimensional subspace:<p>$$Q_h = X W_Q^h \in \mathbb{R}^{n \times d_k}$$<p>Different heads learn to attend to different aspects:<ul><li><strong>Head 1</strong>: Syntactic relationships<li><strong>Head 2</strong>: Semantic similarity<li><strong>Head 3</strong>: Positional patterns<li>etc.</ul><p>The output projection $W_O$ learns to combine these perspectives.<h2 id=tensor-diagram-representation>Tensor Diagram Representation</h2><p>Multi-head attention can be visualized as a tensor network:<pre><code>      X ──┬── W_Q^h ── Q^h ──┐
          │                  ├── Attention ── O^h ──┬── W_O ── Y
          ├── W_K^h ── K^h ──┤                      │
          │                  │                      │
          └── W_V^h ── V^h ──┘                      │
              (for each head h)                     │
                                                    │
      [Concatenate over h] ─────────────────────────┘
</code></pre><h2 id=attention-patterns-across-heads>Attention Patterns Across Heads</h2><p>Different heads often specialize:<table><thead><tr><th>Head Type<th>Pattern<th>Example<tbody><tr><td>Local<td>Attend to nearby tokens<td>"the cat sat"<tr><td>Global<td>Attend to special tokens<td>[CLS], [SEP]<tr><td>Syntactic<td>Attend to syntactic heads<td>Subject-verb<tr><td>Positional<td>Fixed offset patterns<td>Previous token</table><h2 id=code-example>Code Example</h2><pre><code data-lang=python>from attn_tensors.multihead import (
    multihead_attention,
    split_heads,
    combine_heads,
)

# Input: (batch, seq_len, d_model)
X = jnp.randn(2, 10, 64)

# Multi-head attention
Y = multihead_attention(X, X, X, num_heads=8)
# Y.shape = (2, 10, 64)

# Manual head splitting
d_model, num_heads = 64, 8
d_k = d_model // num_heads  # 8

Q = split_heads(X, num_heads)  # (2, 8, 10, 8)
# Now: (batch, heads, seq, d_k)
</code></pre><h2 id=efficient-implementation>Efficient Implementation</h2><h3 id=fused-projections>Fused Projections</h3><p>Instead of separate $W_Q, W_K, W_V$, use a single fused projection:<p>$$[Q; K; V] = X W_{\text{QKV}}$$<p>where $W_{\text{QKV}}$ has shape $d_{model} \times 3d_{model}$.<h3 id=memory-layout>Memory Layout</h3><p>For efficient GPU computation:<ul><li>Store as: <code>(batch, heads, seq, d_k)</code> not <code>(batch, seq, heads, d_k)</code><li>Enables parallel attention computation across heads</ul><h2 id=relation-to-ensemble-methods>Relation to Ensemble Methods</h2><p>Multi-head attention resembles ensemble learning:<ul><li>Each head is an independent "expert"<li>Output projection combines expert opinions<li>Diversity encouraged by random initialization</ul><p>Unlike ensembles, heads share the same input and are trained jointly.</article></main><footer><p>Built with <a href=https://www.getzola.org/ rel=noopener target=_blank>Zola</a> · <a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>Source on GitHub</a></footer>