<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Statistical Mechanics View | Attention as Bilinear Form</title><meta content="A Physicist's Guide to Transformer Attention" name=description><link href=https://planckeon.github.io/attn-as-bilinear-form/style.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css rel=stylesheet><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });" defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><body><header><nav><a class=logo href=https://planckeon.github.io/attn-as-bilinear-form> <span class=logo-icon>∑</span> <span class=logo-text>Attention as Bilinear Form</span> </a><div class=nav-links><a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>GitHub</a></div></nav></header><main><article class="content page"><h1>Statistical Mechanics View</h1><h2 id=softmax-as-gibbs-distribution>Softmax as Gibbs Distribution</h2><p>The attention weights form a <strong>Gibbs distribution</strong> (Boltzmann distribution):<p>$$A^{ij} = \frac{e^{\beta S^{ij}}}{Z^i}$$<p>where:<ul><li>$\beta$ is the <strong>inverse temperature</strong><li>$Z^i = \sum_j e^{\beta S^{ij}}$ is the <strong>partition function</strong></ul><p>In standard attention, $\beta = 1$.<h2 id=temperature-effects>Temperature Effects</h2><p>The temperature $T = 1/\beta$ controls attention sharpness:<table><thead><tr><th>Temperature<th>$\beta$<th>Behavior<tbody><tr><td>High ($T \to \infty$)<td>$\beta \to 0$<td>Uniform attention<tr><td>Normal ($T = 1$)<td>$\beta = 1$<td>Standard attention<tr><td>Low ($T \to 0$)<td>$\beta \to \infty$<td>Hard attention (argmax)</table><h3 id=high-temperature-limit>High Temperature Limit</h3><p>As $\beta \to 0$:<p>$$A^{ij} \to \frac{1}{n_k}$$<p>All keys receive equal attention.<h3 id=low-temperature-limit>Low Temperature Limit</h3><p>As $\beta \to \infty$:<p>$$A^{ij} \to \begin{cases} 1 & \text{if } j = \arg\max_{j'} S^{ij'} \\ 0 & \text{otherwise} \end{cases}$$<p>Only the highest-scoring key receives attention.<h2 id=entropy-of-attention>Entropy of Attention</h2><p>The <strong>entropy</strong> measures attention concentration:<p>$$H^i = -\sum_j A^{ij} \log A^{ij}$$<p>Properties:<ul><li><strong>Maximum entropy</strong> ($H = \log n_k$): Uniform attention<li><strong>Minimum entropy</strong> ($H = 0$): Hard attention on single key<li>Lower entropy = more focused attention</ul><h2 id=free-energy>Free Energy</h2><p>The <strong>free energy</strong> connects entropy and energy:<p>$$F^i = -\frac{1}{\beta} \log Z^i = \langle S^{ij} \rangle - \frac{1}{\beta} H^i$$<p>where $\langle S^{ij} \rangle = \sum_j A^{ij} S^{ij}$ is the expected score.<h2 id=connection-to-hopfield-networks>Connection to Hopfield Networks</h2><p>Ramsauer et al. (2020) showed attention implements a <strong>Hopfield network</strong> update:<p>$$\xi^{\text{new}} = V^T \text{softmax}(\beta K \xi)$$<p>The stored patterns are the rows of $K$. Attention retrieves the pattern most similar to the query.<h3 id=storage-capacity>Storage Capacity</h3><p>Classical Hopfield networks store $\sim 0.14 n$ patterns.<p>Modern (attention-based) Hopfield networks can store <strong>exponentially many</strong> patterns: $\sim e^{d/2}$ for $d$-dimensional patterns.<h2 id=code-example>Code Example</h2><pre><code data-lang=python>from attn_tensors.softmax import (
    softmax_temperature,
    attention_entropy,
    log_partition_function,
)

scores = jnp.randn(10, 20)

# Standard softmax (beta = 1)
A_normal = softmax_temperature(scores, beta=1.0)

# Sharp attention (low temperature)
A_sharp = softmax_temperature(scores, beta=10.0)

# Soft attention (high temperature)
A_soft = softmax_temperature(scores, beta=0.1)

# Compute entropy
H = attention_entropy(A_normal)  # shape: (10,)

# Log partition function
log_Z = log_partition_function(scores, beta=1.0)
</code></pre><h2 id=physical-interpretation>Physical Interpretation</h2><p>Think of attention as a <strong>physical system</strong>:<ul><li><strong>Keys</strong> = possible states<li><strong>Scores</strong> = negative energies (higher score = lower energy = more probable)<li><strong>Temperature</strong> = randomness in state selection<li><strong>Partition function</strong> = normalization over states</ul><p>At low temperature, the system "freezes" into the lowest energy state (highest score). At high temperature, all states are equally likely.<h2 id=energy-based-view>Energy-Based View</h2><h3 id=attention-as-energy-minimization>Attention as Energy Minimization</h3><p>Define an energy function:<p>$$E(i, j) = -S^{ij} = -\frac{1}{\sqrt{d_k}} Q^{ia} K^{ja}$$<p>The attention weight is the Boltzmann probability:<p>$$A^{ij} = \frac{e^{-\beta E(i,j)}}{Z^i} = \frac{e^{\beta S^{ij}}}{Z^i}$$<h3 id=free-energy-1>Free Energy</h3><p>The free energy for query $i$ is:<p>$$F^i = -\frac{1}{\beta} \log Z^i$$<p>This has the familiar form:<p>$$F^i = \langle E \rangle - \frac{1}{\beta} H^i$$<p>where $\langle E \rangle = -\sum_j A^{ij} S^{ij}$ is the expected energy and $H^i$ is the entropy.<h3 id=variational-principle>Variational Principle</h3><p>The attention weights minimize:<p>$$A^* = \arg\min_A \left[ \langle E \rangle - \frac{1}{\beta} H(A) \right]$$<p>subject to $\sum_j A^{ij} = 1$ and $A^{ij} \geq 0$.<p>This is equivalent to softmax!<h2 id=deep-dive-hopfield-networks>Deep Dive: Hopfield Networks</h2><h3 id=classical-hopfield-1982>Classical Hopfield (1982)</h3><p>Energy function:<p>$$E = -\frac{1}{2} \sum_{i,j} W_{ij} s_i s_j$$<p>where $s_i \in {-1, +1}$ and $W_{ij}$ are synaptic weights.<p><strong>Update rule:</strong><p>$$s_i \leftarrow \text{sign}\left(\sum_j W_{ij} s_j\right)$$<p><strong>Storage capacity:</strong> $\sim 0.14 N$ patterns for $N$ neurons.<h3 id=modern-hopfield-ramsauer-et-al-2020>Modern Hopfield (Ramsauer et al., 2020)</h3><p>Energy function:<p>$$E = -\text{lse}(\beta K \xi) + \frac{1}{2}\xi^T \xi + \text{const}$$<p>where $\text{lse}(x) = \log \sum_i e^{x_i}$ is the log-sum-exp.<p><strong>Update rule:</strong><p>$$\xi^{new} = K^T \text{softmax}(\beta K \xi)$$<p>This is exactly attention! The query $\xi$ is updated to be a weighted combination of stored patterns (rows of $K$).<h3 id=why-exponential-capacity>Why Exponential Capacity?</h3><p>Classical Hopfield fails when patterns have overlap (correlation). The error probability grows with pattern density.<p>Modern Hopfield uses exponential separation:<p>$$\text{softmax}(\beta x)_i \approx \begin{cases} 1 & x_i = \max(x) \\ e^{-\beta \Delta} & x_i = \max(x) - \Delta \end{cases}$$<p>For large $\beta$, even small separation $\Delta$ gives clean retrieval.<p><strong>Capacity:</strong> $\sim \exp(d/2)$ patterns in $d$ dimensions!<h3 id=attention-as-associative-memory>Attention as Associative Memory</h3><table><thead><tr><th>Attention<th>Hopfield<tbody><tr><td>Query $Q$<td>Pattern to retrieve<tr><td>Keys $K$<td>Stored patterns<tr><td>Values $V$<td>Pattern outputs<tr><td>Softmax<td>Update rule<tr><td>Output $O$<td>Retrieved pattern</table><h2 id=worked-example-pattern-retrieval>Worked Example: Pattern Retrieval</h2><p><strong>Setup:</strong> Store 3 patterns as keys, retrieve closest to query.<p>$$K = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 0.7 & 0.7 \end{pmatrix}, \quad q = \begin{pmatrix} 0.9 \\ 0.1 \end{pmatrix}$$<p><strong>Step 1: Compute scores</strong><p>$$s = K q = \begin{pmatrix} 0.9 \\ 0.1 \\ 0.7 \end{pmatrix}$$<p><strong>Step 2: Apply softmax</strong> (with $\beta = 1$)<p>$$a = \text{softmax}(s) \approx \begin{pmatrix} 0.48 \\ 0.22 \\ 0.30 \end{pmatrix}$$<p><strong>Step 3: Retrieve pattern</strong><p>$$\xi^{new} = K^T a = \begin{pmatrix} 0.48 + 0.21 \\ 0.22 + 0.21 \end{pmatrix} = \begin{pmatrix} 0.69 \\ 0.43 \end{pmatrix}$$<p>The query moved toward pattern 1 (which it was closest to).<p><strong>With high temperature</strong> ($\beta = 5$):<p>$$a = \text{softmax}(5s) \approx \begin{pmatrix} 0.88 \\ 0.01 \\ 0.11 \end{pmatrix}$$<p>Now retrieval is sharper—almost pure pattern 1.<h2 id=thermodynamic-quantities>Thermodynamic Quantities</h2><h3 id=heat-capacity>Heat Capacity</h3><p>The heat capacity measures sensitivity to temperature:<p>$$C = \frac{\partial \langle E \rangle}{\partial T} = \beta^2 \text{Var}(E)$$<p>High heat capacity near phase transitions—when attention is "deciding" between multiple keys.<h3 id=susceptibility>Susceptibility</h3><p>Response to perturbation in scores:<p>$$\chi^{ij}_{kl} = \frac{\partial A^{ij}}{\partial S^{kl}}$$<p>This is exactly the softmax Jacobian we derived for gradients!<h2 id=connection-to-information-theory>Connection to Information Theory</h2><h3 id=mutual-information>Mutual Information</h3><p>The attention weights encode mutual information:<p>$$I(Q; K) \approx H(A) - H(A|Q)$$<p>where $H(A)$ is the entropy of attention patterns.<h3 id=kl-divergence-and-attention>KL Divergence and Attention</h3><p>The softmax minimizes KL divergence to a uniform prior:<p>$$A^* = \arg\min_A \left[ -\sum_j A^{ij} S^{ij} + \frac{1}{\beta} D_{\text{KL}}(A | U) \right]$$<p>where $U$ is the uniform distribution.</article></main><footer><p>Built with <a href=https://www.getzola.org/ rel=noopener target=_blank>Zola</a> · <a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>Source on GitHub</a></footer>