<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Positional Encodings | Attention as Bilinear Form</title><meta content="A Physicist's Guide to Transformer Attention" name=description><link href=https://planckeon.github.io/attn-as-bilinear-form/style.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css rel=stylesheet><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });" defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><body><header><nav><a class=logo href=https://planckeon.github.io/attn-as-bilinear-form> <span class=logo-icon>∑</span> <span class=logo-text>Attention as Bilinear Form</span> </a><div class=nav-links><a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>GitHub</a></div></nav></header><main><article class="content page"><h1>Positional Encodings</h1><h2 id=why-positional-information>Why Positional Information?</h2><p>Attention is permutation-equivariant: swapping input positions swaps outputs identically. But language has order! "Dog bites man" ≠ "Man bites dog".<p>We need to inject positional information.<h2 id=absolute-positional-encodings>Absolute Positional Encodings</h2><h3 id=sinusoidal-original-transformer>Sinusoidal (Original Transformer)</h3><p>Vaswani et al. (2017) used fixed sinusoidal functions:<p>$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$ $$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$<p><strong>Properties:</strong><ul><li>Each dimension oscillates at different frequency<li>Can extrapolate to longer sequences (in theory)<li>$PE_{pos+k}$ can be represented as linear function of $PE_{pos}$</ul><h3 id=learned-positional-embeddings>Learned Positional Embeddings</h3><p>Simply learn a lookup table:<p>$$X^{ia}_{\text{input}} = X^{ia}_{\text{token}} + P^{ia}$$<p>where $P \in \mathbb{R}^{L_{max} \times d}$ is learned.<p><strong>Tradeoff:</strong><ul><li>More flexible than sinusoidal<li>Cannot extrapolate beyond training length</ul><h2 id=relative-positional-encodings>Relative Positional Encodings</h2><h3 id=the-problem-with-absolute>The Problem with Absolute</h3><p>Absolute encodings conflate content with position. The model must learn that "word at position 5" attending to "word at position 3" is similar to "position 10 attending to position 8".<p>Relative encodings directly encode the offset.<h3 id=t5-style-relative-bias>T5-Style Relative Bias</h3><p>Add a learned bias based on relative position:<p>$$S^{ij} = \frac{1}{\sqrt{d_k}} Q^{ia} K^{ja} + b_{i-j}$$<p>where $b_k$ is a learned bias for relative position $k$.<p><strong>Bucketing:</strong> T5 uses logarithmic bucketing for large offsets:<ul><li>Exact positions for $|k| \leq 8$<li>Bucketed for larger offsets</ul><h3 id=transformer-xl-style>Transformer-XL Style</h3><p>Decompose attention into content and position terms:<p>$$S^{ij} = \underbrace{Q^{ia} K^{ja}}_{\text{content-content}} + \underbrace{Q^{ia} R^{(i-j)a}}_{\text{content-position}} + \underbrace{u^a K^{ja}}_{\text{global content}} + \underbrace{v^a R^{(i-j)a}}_{\text{global position}}$$<p>where:<ul><li>$R^{ka}$: Relative position embeddings<li>$u^a, v^a$: Learned global query vectors</ul><h3 id=alibi-attention-with-linear-biases>ALiBi (Attention with Linear Biases)</h3><p>Press et al. (2022) proposed a simple approach:<p>$$S^{ij} = \frac{1}{\sqrt{d_k}} Q^{ia} K^{ja} - m \cdot |i - j|$$<p>where $m$ is a head-specific slope.<p><strong>Key insight:</strong> No learned positional parameters! Just a linear penalty for distance.<p><strong>Slopes:</strong> Different heads use different slopes: $m_h = 2^{-8h/H}$<table><thead><tr><th>Head<th>Slope<th>Effect<tbody><tr><td>1<td>Large<td>Very local attention<tr><td>H<td>Small<td>Global attention</table><p><strong>Extrapolation:</strong> ALiBi extrapolates well to longer sequences than trained on.<h2 id=rotary-position-embeddings-rope>Rotary Position Embeddings (RoPE)</h2><h3 id=core-idea>Core Idea</h3><p>Su et al. (2021): Encode position by rotating the query/key vectors.<p>For position $m$, rotate by angle $m\theta$:<p>$$f(x, m) = R_m x$$<p>where $R_m$ is a rotation matrix.<h3 id=complex-number-formulation>Complex Number Formulation</h3><p>For 2D, think of $(q_1, q_2)$ as complex number $q_1 + iq_2$:<p>$$f(q, m) = q \cdot e^{im\theta} = (q_1 + iq_2)(\cos m\theta + i\sin m\theta)$$<p>Expanding: $$\text{Re}[f(q,m)] = q_1 \cos m\theta - q_2 \sin m\theta$$ $$\text{Im}[f(q,m)] = q_1 \sin m\theta + q_2 \cos m\theta$$<h3 id=block-diagonal-rotation>Block-Diagonal Rotation</h3><p>For $d$-dimensional vectors, apply 2D rotations to pairs:<p>$$R_m = \begin{pmatrix} \cos m\theta_1 & -\sin m\theta_1 & & & \\ \sin m\theta_1 & \cos m\theta_1 & & & \\ & & \cos m\theta_2 & -\sin m\theta_2 & \\ & & \sin m\theta_2 & \cos m\theta_2 & \\ & & & & \ddots \end{pmatrix}$$<p>Different frequencies: $\theta_i = 10000^{-2i/d}$<h3 id=key-property>Key Property</h3><p>The dot product depends only on relative position:<p>$$(R_m q)^T (R_n k) = q^T R_m^T R_n k = q^T R_{n-m} k$$<p>This is because rotations compose: $R_m^T R_n = R_{n-m}$.<h3 id=in-index-notation>In Index Notation</h3><p>For query $Q^{ia}$ at position $i$:<p>$$\tilde{Q}^{ia} = R^{ab}(i) Q^{ib}$$<p>Score computation:<p>$$S^{ij} = \frac{1}{\sqrt{d_k}} \tilde{Q}^{ia} \tilde{K}^{ja} = \frac{1}{\sqrt{d_k}} R^{ab}(i) Q^{ib} R^{ac}(j) K^{jc}$$<p>Using $R^T(i) R(j) = R(j-i)$:<p>$$S^{ij} = \frac{1}{\sqrt{d_k}} Q^{ia} R^{ab}(j-i) K^{jb}$$<h3 id=efficient-implementation>Efficient Implementation</h3><p>No explicit matrix multiplication needed! Use:<p>$$\tilde{q} = q \odot \cos(m\theta) + \text{rotate_half}(q) \odot \sin(m\theta)$$<p>where <code>rotate_half</code> swaps and negates pairs:<pre><code>rotate_half([q1, q2, q3, q4, ...]) = [-q2, q1, -q4, q3, ...]
</code></pre><h3 id=gradients-for-rope>Gradients for RoPE</h3><p>Since RoPE is just multiplication by rotation matrices:<p>$$\frac{\partial L}{\partial Q^{ia}} = R^{ab}(i) \frac{\partial L}{\partial \tilde{Q}^{ib}}$$<p>The rotation is its own transpose (orthogonal), so gradients just rotate back.<h2 id=comparison-table>Comparison Table</h2><table><thead><tr><th>Method<th>Learnable<th>Extrapolation<th>Relative<th>Memory<tbody><tr><td>Sinusoidal<td>No<td>Moderate<td>No<td>O(L·d)<tr><td>Learned<td>Yes<td>Poor<td>No<td>O(L·d)<tr><td>T5 Bias<td>Yes<td>Moderate<td>Yes<td>O(L²)<tr><td>ALiBi<td>No<td>Excellent<td>Yes<td>O(1)<tr><td>RoPE<td>No<td>Good<td>Yes<td>O(L·d)</table><h2 id=code-example>Code Example</h2><pre><code data-lang=python>from attn_tensors.positional import (
    sinusoidal_encoding,
    rotary_embedding,
    apply_rope,
    alibi_bias,
)

seq_len, d_model = 100, 64

# Sinusoidal
pos_enc = sinusoidal_encoding(seq_len, d_model)
X = X + pos_enc

# RoPE
Q_rotated = apply_rope(Q, positions)
K_rotated = apply_rope(K, positions)
scores = Q_rotated @ K_rotated.T / jnp.sqrt(d_k)

# ALiBi
scores = Q @ K.T / jnp.sqrt(d_k)
scores = scores + alibi_bias(seq_len, num_heads)
</code></pre><h2 id=rope-worked-example>RoPE: Worked Example</h2><p><strong>Setup:</strong> $d = 4$, position $m = 2$, $\theta = [1.0, 0.1]$<p><strong>Query:</strong> $q = [1, 0, 1, 0]$<p><strong>Rotation angles:</strong> $m\theta = [2.0, 0.2]$<p><strong>Apply rotation to pairs:</strong><p>Pair 1: $(q_1, q_2) = (1, 0)$ $$\tilde{q}_1 = 1 \cdot \cos(2) - 0 \cdot \sin(2) = \cos(2) \approx -0.42$$ $$\tilde{q}_2 = 1 \cdot \sin(2) + 0 \cdot \cos(2) = \sin(2) \approx 0.91$$<p>Pair 2: $(q_3, q_4) = (1, 0)$ $$\tilde{q}_3 = 1 \cdot \cos(0.2) - 0 \cdot \sin(0.2) \approx 0.98$$ $$\tilde{q}_4 = 1 \cdot \sin(0.2) + 0 \cdot \cos(0.2) \approx 0.20$$<p><strong>Result:</strong> $\tilde{q} \approx [-0.42, 0.91, 0.98, 0.20]$</article></main><footer><p>Built with <a href=https://www.getzola.org/ rel=noopener target=_blank>Zola</a> · <a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>Source on GitHub</a></footer>