<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Attention as Bilinear Form</title><meta content="A Physicist's Guide to Transformer Attention" name=description><link href=https://planckeon.github.io/attn-as-bilinear-form/style.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css rel=stylesheet><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js></script><script onload="renderMathInElement(document.body, {
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false}
            ]
        });" defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js></script><body><header><nav><a class=logo href=https://planckeon.github.io/attn-as-bilinear-form> <span class=logo-icon>∑</span> <span class=logo-text>Attention as Bilinear Form</span> </a><div class=nav-links><a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>GitHub</a></div></nav></header><main><article class="content home"><h1 id=attention-as-bilinear-form>Attention as Bilinear Form</h1><p><em>A Physicist's Guide to Transformer Attention using Tensor Calculus</em><hr><h2 id=the-core-insight>The Core Insight</h2><p>The attention mechanism in transformers can be understood through the lens of <strong>tensor calculus</strong> and <strong>differential geometry</strong>. This perspective reveals deep connections to physics and provides a rigorous mathematical foundation.<p><strong>Standard attention formula:</strong><p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$<p><strong>In index notation:</strong><p>$$O^{ib} = A^{ij} V^{jb}, \quad A^{ij} = \frac{\exp(S^{ij})}{\sum_k \exp(S^{ik})}, \quad S^{ij} = \frac{1}{\sqrt{d_k}} Q^{ia} K^{ja}$$<hr><h2 id=key-perspectives>Key Perspectives</h2><h3 id=1-bilinear-forms-and-metric-tensors>1. Bilinear Forms and Metric Tensors</h3><p>The score computation is a <strong>bilinear form</strong>:<p>$$S^{ij} = Q^{ia} g_{ab} K^{jb}$$<p>where $g_{ab} = \frac{1}{\sqrt{d_k}} \delta_{ab}$ is the <strong>metric tensor</strong>. This gives us:<ul><li>A geometric interpretation of similarity<li>A framework for learned metrics<li>Connection to Riemannian geometry</ul><h3 id=2-softmax-as-gibbs-distribution>2. Softmax as Gibbs Distribution</h3><p>The attention weights form a <strong>Gibbs distribution</strong> from statistical mechanics:<p>$$A^{ij} = \frac{e^{\beta S^{ij}}}{Z^i}, \quad Z^i = \sum_j e^{\beta S^{ij}}$$<p>where $\beta = 1$ is the inverse temperature. This reveals:<ul><li><strong>High temperature</strong> ($\beta \to 0$): Uniform attention<li><strong>Low temperature</strong> ($\beta \to \infty$): Hard attention (argmax)<li><strong>Entropy</strong> measures attention concentration</ul><h3 id=3-hopfield-network-interpretation>3. Hopfield Network Interpretation</h3><p>Modern Hopfield networks show attention is an <strong>associative memory</strong>:<p>$$\xi^{\text{new}} = V^T \cdot \text{softmax}(\beta \cdot K \cdot \xi)$$<p>The patterns stored in $K$ are retrieved via the attention mechanism.<hr><h2 id=gradient-derivations>Gradient Derivations</h2><p>Using index notation, we derive all gradients explicitly:<p><strong>Gradient w.r.t. Queries:</strong> $$\frac{\partial L}{\partial Q^{kl}} = \frac{1}{\sqrt{d_k}} \frac{\partial L}{\partial S^{kj}} K^{jl}$$<p><strong>Gradient through Softmax:</strong> $$\frac{\partial L}{\partial S^{ij}} = A^{ij} \left( \frac{\partial L}{\partial A^{ij}} - \sum_{j'} A^{ij'} \frac{\partial L}{\partial A^{ij'}} \right)$$<p><strong>Gradient w.r.t. Values:</strong> $$\frac{\partial L}{\partial V^{kl}} = A^{ik} \frac{\partial L}{\partial O^{il}}$$<p>All gradients are verified against JAX autodiff.<hr><h2 id=quick-start>Quick Start</h2><pre><code data-lang=python>import jax.numpy as jnp
from attn_tensors import scaled_dot_product_attention
from attn_tensors.bilinear import bilinear_form_batch, scaled_euclidean_metric

# Standard attention
Q = jnp.array([[1.0, 0.0], [0.0, 1.0]])
K = jnp.array([[1.0, 0.0], [0.5, 0.5], [0.0, 1.0]])
V = jnp.array([[1.0, 0.0], [0.5, 0.5], [0.0, 1.0]])

output = scaled_dot_product_attention(Q, K, V)

# With explicit metric tensor
g = scaled_euclidean_metric(d=2)
scores = bilinear_form_batch(Q, K, g)
</code></pre><hr><h2 id=installation>Installation</h2><pre><code data-lang=bash># Clone the repository
git clone https://github.com/planckeon/attn-as-bilinear-form
cd attn-as-bilinear-form

# Install with uv
uv sync

# Run tests
uv run pytest tests/ -v
</code></pre><hr><h2 id=modules>Modules</h2><table><thead><tr><th>Module<th>Description<tbody><tr><td><code>attention</code><td>Core attention operations<tr><td><code>bilinear</code><td>Metric tensors and bilinear forms<tr><td><code>gradients</code><td>Manual gradient derivations<tr><td><code>softmax</code><td>Softmax, entropy, Gibbs distribution<tr><td><code>multihead</code><td>Multi-head attention<tr><td><code>masking</code><td>Causal and padding masks<tr><td><code>hopfield</code><td>Hopfield network interpretation</table><hr><h2 id=theory-deep-dives>Theory Deep Dives</h2><ul><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/bilinear/>Bilinear Forms and Metrics</a><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/attention/>Attention Mechanism</a><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/gradients/>Gradient Derivations</a><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/statistical/>Statistical Mechanics View</a><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/multihead/>Multi-Head Attention</a><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/positional/>Positional Encodings</a><li><a href=https://planckeon.github.io/attn-as-bilinear-form/theory/efficient/>Efficient Attention</a></ul><hr><h2 id=references>References</h2><ol><li>Vaswani et al. (2017). <em>Attention Is All You Need</em><li>Ramsauer et al. (2020). <em>Hopfield Networks is All You Need</em><li>Amari (1998). <em>Natural Gradient Works Efficiently in Learning</em></ol></article></main><footer><p>Built with <a href=https://www.getzola.org/ rel=noopener target=_blank>Zola</a> · <a href=https://github.com/planckeon/attn-as-bilinear-form rel=noopener target=_blank>Source on GitHub</a></footer>